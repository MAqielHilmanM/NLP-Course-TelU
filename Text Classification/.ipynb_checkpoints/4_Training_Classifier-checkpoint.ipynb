{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#Try different classifier model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataframe\n",
    "path_df = \"Data/df.pickle\"\n",
    "with open(path_df, 'rb') as data:\n",
    "    df = pickle.load(data)\n",
    "\n",
    "# features_train\n",
    "path_features_train = \"Data/features_train.pickle\"\n",
    "with open(path_features_train, 'rb') as data:\n",
    "    features_train = pickle.load(data)\n",
    "\n",
    "# labels_train\n",
    "path_labels_train = \"Data/labels_train.pickle\"\n",
    "with open(path_labels_train, 'rb') as data:\n",
    "    labels_train = pickle.load(data)\n",
    "\n",
    "# features_test\n",
    "path_features_test = \"Data/features_test.pickle\"\n",
    "with open(path_features_test, 'rb') as data:\n",
    "    features_test = pickle.load(data)\n",
    "\n",
    "# labels_test\n",
    "path_labels_test = \"Data/labels_test.pickle\"\n",
    "with open(path_labels_test, 'rb') as data:\n",
    "    labels_test = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1891, 300)\n",
      "(334, 300)\n"
     ]
    }
   ],
   "source": [
    "print(features_train.shape)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {1:'Logistic Regression',\n",
    "          2:'Multinomial Naive Bayes', \n",
    "          3:'K Nearest Neighbour', \n",
    "          4:'Support Vector Machines', \n",
    "          5:'Random Forest'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use in Logistic Regression:\n",
      "\n",
      "{'C': 1.0,\n",
      " 'class_weight': None,\n",
      " 'dual': False,\n",
      " 'fit_intercept': True,\n",
      " 'intercept_scaling': 1,\n",
      " 'l1_ratio': None,\n",
      " 'max_iter': 100,\n",
      " 'multi_class': 'auto',\n",
      " 'n_jobs': None,\n",
      " 'penalty': 'l2',\n",
      " 'random_state': 8,\n",
      " 'solver': 'lbfgs',\n",
      " 'tol': 0.0001,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# Pilihan jenis classifier. Untuk selain nomor 1, maka perlu penyesuaian di bagian Random Search dan Grid Search.\n",
    "\n",
    "choice = 1\n",
    "\n",
    "if choice == 1:\n",
    "    classifier = LogisticRegression(random_state = 8)\n",
    "    print('Parameters currently in use in {}:\\n'.format(models[choice]))\n",
    "    pprint(classifier.get_params())\n",
    "elif choice==2:\n",
    "    classifier = MultinomialNB()\n",
    "    print('Parameters currently in use in {}:\\n'.format(models[choice]))\n",
    "    print(classifier)\n",
    "elif choice==3:\n",
    "    classifier =KNeighborsClassifier()\n",
    "    print('Parameters currently in use in {}:\\n'.format(models[choice]))\n",
    "    pprint(classifier.get_params())\n",
    "elif choice==4:\n",
    "    classifier =svm.SVC(random_state=8)\n",
    "    print('Parameters currently in use in {}:\\n'.format(models[choice]))\n",
    "    pprint(classifier.get_params())\n",
    "elif choice==5:\n",
    "    classifier = RandomForestClassifier(random_state = 8)\n",
    "    print('Parameters currently in use in {}:\\n'.format(models[choice]))\n",
    "    pprint(classifier.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Search Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cari parameter yang secara random menggunakan cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': [0.1,\n",
      "       0.3,\n",
      "       0.5,\n",
      "       0.7,\n",
      "       0.8999999999999999,\n",
      "       1.0999999999999999,\n",
      "       1.3,\n",
      "       1.5,\n",
      "       1.7,\n",
      "       1.9],\n",
      " 'class_weight': ['balanced', None],\n",
      " 'multi_class': ['multinomial'],\n",
      " 'penalty': ['l2'],\n",
      " 'solver': ['newton-cg', 'sag', 'saga', 'lbfgs']}\n"
     ]
    }
   ],
   "source": [
    "if choice == 1:\n",
    "    # Create the random grid logistic regression\n",
    "    random_grid = {'C': [float(x) for x in np.linspace(start = 0.1, stop = 1.9, num = 10)],\n",
    "               'multi_class': ['multinomial'],\n",
    "               'solver': ['newton-cg', 'sag', 'saga', 'lbfgs'],\n",
    "               'class_weight': ['balanced', None],\n",
    "               'penalty': ['l2']}\n",
    "elif choice==2:\n",
    "    pass\n",
    "elif choice==3:\n",
    "    pass\n",
    "elif choice==4:\n",
    "    # Create the random grid SVM\n",
    "    random_grid = {'C': [.0001, .001, .01],\n",
    "                  'kernel': ['linear', 'rbf', 'poly'],\n",
    "                  'gamma': [.0001, .001, .01, .1, 1, 10, 100],\n",
    "                  'degree': [1, 2, 3, 4, 5],\n",
    "                  'probability': [True]\n",
    "                 }\n",
    "elif choice==5:\n",
    "    # Create the random grid Random Forest\n",
    "    random_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)],\n",
    "               'max_features': ['auto', 'sqrt'],\n",
    "               'max_depth': [20, 40, 60, 80, 100, None],\n",
    "               'min_samples_split': [2, 5, 10],\n",
    "               'min_samples_leaf': [1, 2, 4],\n",
    "               'bootstrap': [True, False]\n",
    "                     }\n",
    "    \n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:   36.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                dual=False, fit_intercept=True,\n",
       "                                                intercept_scaling=1,\n",
       "                                                l1_ratio=None, max_iter=100,\n",
       "                                                multi_class='auto', n_jobs=None,\n",
       "                                                penalty='l2', random_state=8,\n",
       "                                                solver='lbfgs', tol=0.0001,\n",
       "                                                verbose=0, warm_start=False),\n",
       "                   iid='deprecated', n_iter=50, n_jobs=None,\n",
       "                   param_distributions={'C': [0.1, 0.3, 0.5, 0.7,\n",
       "                                              0.8999999999999999,\n",
       "                                              1.0999999999999999, 1.3, 1.5, 1.7,\n",
       "                                              1.9],\n",
       "                                        'class_weight': ['balanced', None],\n",
       "                                        'multi_class': ['multinomial'],\n",
       "                                        'penalty': ['l2'],\n",
       "                                        'solver': ['newton-cg', 'sag', 'saga',\n",
       "                                                   'lbfgs']},\n",
       "                   pre_dispatch='2*n_jobs', random_state=8, refit=True,\n",
       "                   return_train_score=False, scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definition of the random search\n",
    "random_search = RandomizedSearchCV(estimator=classifier,\n",
    "                                   param_distributions=random_grid,\n",
    "                                   n_iter=50,\n",
    "                                   scoring='accuracy',\n",
    "                                   cv=3, \n",
    "                                   verbose=1, \n",
    "                                   random_state=8)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters from Random Search are:\n",
      "{'solver': 'sag', 'penalty': 'l2', 'multi_class': 'multinomial', 'class_weight': 'balanced', 'C': 1.9}\n",
      "\n",
      "The mean accuracy of a model with these hyperparameters is:\n",
      "0.9587527985309285\n"
     ]
    }
   ],
   "source": [
    "print(\"The best hyperparameters from Random Search are:\")\n",
    "print(random_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kemudian lanjutkan pencarian yang lebih detil terhadap daerah nilai terbaik hasil random search di atas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {'C': [float(x) for x in np.linspace(start = 0.6, stop = 1.5, num = 10)],\n",
    "               'multi_class': ['multinomial'],\n",
    "               'solver': ['sag'],\n",
    "               'class_weight': ['balanced']}\n",
    "\n",
    "# Create a base model\n",
    "classifier = LogisticRegression(random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:   10.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=3, random_state=8, test_size=0.33, train_size=None),\n",
       "             error_score=nan,\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='auto',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=8, solver='lbfgs',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'C': [0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2000000000000002,\n",
       "                               1.3, 1.4, 1.5],\n",
       "                         'class_weight': ['balanced'],\n",
       "                         'multi_class': ['multinomial'], 'solver': ['sag']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually create the splits in CV in order to be able to fix a random_state (GridSearchCV doesn't have that argument)\n",
    "cv_sets = ShuffleSplit(n_splits = 3, test_size = .33, random_state = 8)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=classifier, \n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=cv_sets,\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters from Grid Search are:\n",
      "{'C': 1.4, 'class_weight': 'balanced', 'multi_class': 'multinomial', 'solver': 'sag'}\n",
      "\n",
      "The mean accuracy of a model with these hyperparameters is:\n",
      "0.9696000000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"The best hyperparameters from Grid Search are:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.4, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='multinomial', n_jobs=None,\n",
       "                   penalty='l2', random_state=8, solver='sag', tol=0.0001,\n",
       "                   verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_classifier = grid_search.best_estimator_\n",
    "\n",
    "best_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fit and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.4, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='multinomial', n_jobs=None,\n",
       "                   penalty='l2', random_state=8, solver='sag', tol=0.0001,\n",
       "                   verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_classifier.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_pred = best_classifier.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training Set Accuracy</th>\n",
       "      <th>Test Set Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.985722</td>\n",
       "      <td>0.94012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Training Set Accuracy  Test Set Accuracy\n",
       "0  Logistic Regression               0.985722            0.94012"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\n",
    "     'Model': 'Logistic Regression',\n",
    "     'Training Set Accuracy': accuracy_score(labels_train, best_classifier.predict(features_train)),\n",
    "     'Test Set Accuracy': accuracy_score(labels_test, classifier_pred)\n",
    "}\n",
    "\n",
    "df_models = pd.DataFrame(d, index=[0])\n",
    "df_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93        81\n",
      "           1       0.90      0.96      0.93        49\n",
      "           2       0.96      0.89      0.92        72\n",
      "           3       0.99      0.99      0.99        72\n",
      "           4       0.93      0.92      0.92        60\n",
      "\n",
      "    accuracy                           0.94       334\n",
      "   macro avg       0.94      0.94      0.94       334\n",
      "weighted avg       0.94      0.94      0.94       334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "print(\"Classification report\")\n",
    "print(classification_report(labels_test,classifier_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 921.6x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "aux_df = df[['Category', 'Category_Code']].drop_duplicates().sort_values('Category_Code')\n",
    "conf_matrix = confusion_matrix(labels_test, classifier_pred)\n",
    "plt.figure(figsize=(12.8,6))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True,\n",
    "            xticklabels=aux_df['Category'].values, \n",
    "            yticklabels=aux_df['Category'].values,\n",
    "            cmap=\"Blues\")\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlabel('Actual')\n",
    "plt.title('Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bandingkan performansi dengan base model, yaitu model dengan parameter default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9401197604790419"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = LogisticRegression(random_state = 8)\n",
    "base_model.fit(features_train, labels_train)\n",
    "accuracy_score(labels_test, base_model.predict(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9401197604790419"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_classifier.fit(features_train, labels_train)\n",
    "accuracy_score(labels_test, best_classifier.predict(features_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latihan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ubah format penyimpanan data ke CSV\n",
    "2. Coba buatkan feature berikut (save dan upload feature), lalu laporkan pengaruhnya terhadap akurasi klasifikasi:\n",
    "    a. Tanpa proses normalisation\n",
    "    b. Tanpa proses lemmatisation\n",
    "    c. Tanpa menghilangkan stopwords\n",
    "3. Coba buat tfidf dengan nilai \"max_features\" yang berbeda-beda (lebih besar dan lebih kecil dari 300), lalu laporkan pengaruhnya terhadap akurasi klasifikasi.\n",
    "4. Coba dengan beberapa algoritma klasifikasi yang berbeda (minimal 2 algoritma), carilah parameter terbaik (jelaskan nilai2 parameter yang telah dicoba untuk tiap jenis algoritma).\n",
    "5. Jika anda ingin menggunakan teks bahasa Indonesia, bagian mana saja yang perlu dilakukan penyesuaian?\n",
    "6. Opsional: Gunakan word embedding (e.g word2vec, GloVe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jawaban"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jawaban ditulis dalam sebuah laporan singkat format pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import csv\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writePickleToCsv(csv_path, pickle_path):\n",
    "    panda_df = pd.read_pickle(pickle_path)\n",
    "    try:\n",
    "        panda_df.to_csv(csv_path, sep=';',index=False, encoding='utf-8')\n",
    "    except:\n",
    "        new_panda_df = pd.DataFrame(panda_df)\n",
    "        new_panda_df.to_csv(csv_path, sep=';',index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe to csv (df)\n",
    "path_df_csv = \"Data/df.csv\"\n",
    "writePickleToCsv(path_df_csv,path_df)\n",
    "\n",
    "# # features_train to csv (features_train)\n",
    "path_features_train_csv = \"Data/features_train.csv\"\n",
    "writePickleToCsv(path_features_train_csv,path_features_train)\n",
    "\n",
    "# labels_train to csv (labels_train)\n",
    "path_labels_train_csv = \"Data/labels_train.csv\"\n",
    "writePickleToCsv(path_labels_train_csv,path_labels_train)\n",
    "\n",
    "\n",
    "# features_test to csv (features_test)\n",
    "path_features_test_csv = \"Data/features_test.csv\"\n",
    "writePickleToCsv(path_features_test_csv,path_features_test)\n",
    "\n",
    "\n",
    "# labels_test to csv (labels_test)\n",
    "path_labels_test_csv = \"Data/labels_test.csv\"\n",
    "writePickleToCsv(path_labels_test_csv,path_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\razeluxe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\razeluxe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\razeluxe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# download some library\n",
    "\n",
    "# Downloading punkt and wordnet from NLTK\n",
    "nltk.download('punkt')\n",
    "print(\"------------------------------------------------------------\")\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Saving the lemmatizer into an object\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Downloading the stop words list\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Loading the stop words in english\n",
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmilized text list method\n",
    "def lemillizedTextList(dataframe, content_key):\n",
    "    nrows = len(dataframe)\n",
    "    lemmatized_text_list = []\n",
    "\n",
    "    for row in range(0, nrows):\n",
    "\n",
    "        # Create an empty list containing lemmatized words\n",
    "        lemmatized_list = []\n",
    "\n",
    "        # Save the text and its words into an object\n",
    "        text = dataframe.loc[row][content_key]\n",
    "        text_words = text.split(\" \")\n",
    "\n",
    "        # Iterate through every word to lemmatize\n",
    "        for word in text_words:\n",
    "            lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "\n",
    "        # Join the list\n",
    "        lemmatized_text = \" \".join(lemmatized_list)\n",
    "\n",
    "        # Append to the list containing the texts\n",
    "        lemmatized_text_list.append(lemmatized_text)\n",
    "        \n",
    "    return lemmatized_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'business' category:\n",
      "  . Most correlated unigrams:\n",
      ". market\n",
      ". bank\n",
      ". firm\n",
      ". economy\n",
      ". growth\n",
      "  . Most correlated bigrams:\n",
      ". last year\n",
      ". year old\n",
      "\n",
      "# 'entertainment' category:\n",
      "  . Most correlated unigrams:\n",
      ". TV\n",
      ". best\n",
      ". award\n",
      ". star\n",
      ". film\n",
      "  . Most correlated bigrams:\n",
      ". tell BBC\n",
      ". Mr Blair\n",
      "\n",
      "# 'politics' category:\n",
      "  . Most correlated unigrams:\n",
      ". minister\n",
      ". party\n",
      ". Blair\n",
      ". election\n",
      ". Labour\n",
      "  . Most correlated bigrams:\n",
      ". tell BBC\n",
      ". Mr Blair\n",
      "\n",
      "# 'sport' category:\n",
      "  . Most correlated unigrams:\n",
      ". club\n",
      ". side\n",
      ". game\n",
      ". team\n",
      ". match\n",
      "  . Most correlated bigrams:\n",
      ". say Mr\n",
      ". year old\n",
      "\n",
      "# 'tech' category:\n",
      "  . Most correlated unigrams:\n",
      ". Microsoft\n",
      ". computer\n",
      ". software\n",
      ". technology\n",
      ". users\n",
      "  . Most correlated bigrams:\n",
      ". year old\n",
      ". say Mr\n",
      "\n",
      "Bigram : \n",
      "['tell BBC', 'said The', 'last year', 'Mr Blair', 'year old', 'say Mr']\n"
     ]
    }
   ],
   "source": [
    "# feature without normalisation (wn)\n",
    "df_wn = df\n",
    "\n",
    "# lemillized process\n",
    "df_wn['Content_Parsed_5'] = lemillizedTextList(df_wn,'Content')\n",
    "\n",
    "# stopword process\n",
    "df_wn['Content_Parsed_6'] = df_wn['Content_Parsed_5']\n",
    "\n",
    "for stop_word in stop_words:\n",
    "\n",
    "    regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "    df_wn['Content_Parsed_6'] = df_wn['Content_Parsed_6'].str.replace(regex_stopword, '')\n",
    "\n",
    "# reformat list\n",
    "list_columns = [\"File_Name\", \"Category\", \"Content\", \"Content_Parsed_6\"]\n",
    "df_wn = df_wn[list_columns]\n",
    "\n",
    "df_wn = df_wn.rename(columns={'Content_Parsed_6': 'Content_Parsed'})\n",
    "\n",
    "# labeling \n",
    "category_codes = {\n",
    "    'business': 0,\n",
    "    'entertainment': 1,\n",
    "    'politics': 2,\n",
    "    'sport': 3,\n",
    "    'tech': 4\n",
    "}\n",
    "\n",
    "# Category mapping\n",
    "df_wn['Category_Code'] = df_wn['Category']\n",
    "df_wn = df_wn.replace({'Category_Code':category_codes})\n",
    "\n",
    "# Split Data to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_wn['Content_Parsed'], \n",
    "                                                    df_wn['Category_Code'], \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=8)\n",
    "# Parameter election\n",
    "ngram_range = (1,2)\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 300\n",
    "\n",
    "# create tfId\n",
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "\n",
    "# define feature train\n",
    "features_train = tfidf.fit_transform(X_train).toarray()\n",
    "labels_train = y_train\n",
    "\n",
    "# define feature test\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "labels_test = y_test\n",
    "\n",
    "# define unigram and bigram\n",
    "for Product, category_id in sorted(category_codes.items()):\n",
    "    features_chi2 = chi2(features_train, labels_train == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    print(\"# '{}' category:\".format(Product))\n",
    "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-5:])))\n",
    "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-2:])))\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Bigram : \")\n",
    "print(bigrams)\n",
    "\n",
    "# Saved output \n",
    "# X_train\n",
    "filename = 'Data/WN/X_train.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(X_train, output)\n",
    "    \n",
    "# X_test    \n",
    "filename = 'Data/WN/X_test.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(X_test, output)\n",
    "    \n",
    "# y_train\n",
    "filename = 'Data/WN/y_train.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(y_train, output)\n",
    "    \n",
    "# y_test\n",
    "filename = 'Data/WN/y_test.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(y_test, output)\n",
    "    \n",
    "# df\n",
    "filename = 'Data/WN/df_wn.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(df_wn, output)\n",
    "    \n",
    "# features_train\n",
    "filename = 'Data/WN/features_train.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(features_train, output)\n",
    "\n",
    "# labels_train\n",
    "filename = 'Data/WN/labels_train.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(labels_train, output)\n",
    "\n",
    "# features_test\n",
    "filename = 'Data/WN/features_test.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(features_test, output)\n",
    "\n",
    "# labels_test\n",
    "filename = 'Data/WN/labels_test.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(labels_test, output)\n",
    "    \n",
    "# TF-IDF object\n",
    "filename = 'Data/WN/tfidf.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(tfidf, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'business' category:\n",
      "  . Most correlated unigrams:\n",
      ". prices\n",
      ". economy\n",
      ". bank\n",
      ". growth\n",
      ". oil\n",
      "  . Most correlated bigrams:\n",
      ". last year\n",
      ". year old\n",
      "\n",
      "# 'entertainment' category:\n",
      "  . Most correlated unigrams:\n",
      ". films\n",
      ". star\n",
      ". award\n",
      ". awards\n",
      ". film\n",
      "  . Most correlated bigrams:\n",
      ". mr blair\n",
      ". prime minister\n",
      "\n",
      "# 'politics' category:\n",
      "  . Most correlated unigrams:\n",
      ". tory\n",
      ". blair\n",
      ". election\n",
      ". party\n",
      ". labour\n",
      "  . Most correlated bigrams:\n",
      ". prime minister\n",
      ". mr blair\n",
      "\n",
      "# 'sport' category:\n",
      "  . Most correlated unigrams:\n",
      ". side\n",
      ". game\n",
      ". season\n",
      ". match\n",
      ". cup\n",
      "  . Most correlated bigrams:\n",
      ". said mr\n",
      ". year old\n",
      "\n",
      "# 'tech' category:\n",
      "  . Most correlated unigrams:\n",
      ". digital\n",
      ". technology\n",
      ". computer\n",
      ". software\n",
      ". users\n",
      "  . Most correlated bigrams:\n",
      ". year old\n",
      ". said mr\n",
      "\n",
      "Bigram : \n",
      "['told bbc', 'last year', 'mr brown', 'prime minister', 'mr blair', 'year old', 'said mr']\n"
     ]
    }
   ],
   "source": [
    "# feature without lemmatisation (wl)\n",
    "df_wl = df\n",
    "\n",
    "# normalisation process\n",
    "df_wl['Content_Parsed_1'] = df_wl['Content'].str.replace(\"\\r\", \" \")\n",
    "df_wl['Content_Parsed_1'] = df_wl['Content_Parsed_1'].str.replace(\"\\n\", \" \")\n",
    "df_wl['Content_Parsed_1'] = df_wl['Content_Parsed_1'].str.replace(\"    \", \" \")\n",
    "df_wl['Content_Parsed_1'] = df_wl['Content_Parsed_1'].str.replace('\"', '')\n",
    "df_wl['Content_Parsed_2'] = df_wl['Content_Parsed_1'].str.lower()\n",
    "punctuation_signs = list(\"?:!.,;\")\n",
    "df_wl['Content_Parsed_3'] = df_wl['Content_Parsed_2']\n",
    "for punct_sign in punctuation_signs:\n",
    "    df_wl['Content_Parsed_3'] = df_wl['Content_Parsed_3'].str.replace(punct_sign, '')\n",
    "df_wl['Content_Parsed_4'] = df_wl['Content_Parsed_3'].str.replace(\"'s\", \"\")\n",
    "\n",
    "# stopword process\n",
    "df_wl['Content_Parsed_6'] = df_wl['Content_Parsed_4']\n",
    "\n",
    "for stop_word in stop_words:\n",
    "\n",
    "    regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "    df_wl['Content_Parsed_6'] = df_wl['Content_Parsed_6'].str.replace(regex_stopword, '')\n",
    "\n",
    "# reformat list\n",
    "list_columns = [\"File_Name\", \"Category\", \"Content\", \"Content_Parsed_6\"]\n",
    "df_wl = df_wl[list_columns]\n",
    "\n",
    "df_wl = df_wl.rename(columns={'Content_Parsed_6': 'Content_Parsed'})\n",
    "\n",
    "# labeling \n",
    "category_codes = {\n",
    "    'business': 0,\n",
    "    'entertainment': 1,\n",
    "    'politics': 2,\n",
    "    'sport': 3,\n",
    "    'tech': 4\n",
    "}\n",
    "\n",
    "# Category mapping\n",
    "df_wl['Category_Code'] = df_wl['Category']\n",
    "df_wl = df_wl.replace({'Category_Code':category_codes})\n",
    "\n",
    "# Split Data to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_wl['Content_Parsed'], \n",
    "                                                    df_wl['Category_Code'], \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=8)\n",
    "# Parameter election\n",
    "ngram_range = (1,2)\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 300\n",
    "\n",
    "# create tfId\n",
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "\n",
    "# define feature train\n",
    "features_train = tfidf.fit_transform(X_train).toarray()\n",
    "labels_train = y_train\n",
    "\n",
    "# define feature test\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "labels_test = y_test\n",
    "\n",
    "# define unigram and bigram\n",
    "for Product, category_id in sorted(category_codes.items()):\n",
    "    features_chi2 = chi2(features_train, labels_train == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    print(\"# '{}' category:\".format(Product))\n",
    "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-5:])))\n",
    "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-2:])))\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Bigram : \")\n",
    "print(bigrams)\n",
    "    \n",
    "# Saved output \n",
    "# X_train\n",
    "filename = 'Data/WL/X_train.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(X_train, output)\n",
    "    \n",
    "# X_test    \n",
    "filename = 'Data/WL/X_test.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(X_test, output)\n",
    "    \n",
    "# y_train\n",
    "filename = 'Data/WL/y_train.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(y_train, output)\n",
    "    \n",
    "# y_test\n",
    "filename = 'Data/WL/y_test.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(y_test, output)\n",
    "    \n",
    "# df\n",
    "filename = 'Data/WL/df_wl.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(df_wl, output)\n",
    "    \n",
    "# features_train\n",
    "filename = 'Data/WL/features_train.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(features_train, output)\n",
    "\n",
    "# labels_train\n",
    "filename = 'Data/WL/labels_train.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(labels_train, output)\n",
    "\n",
    "# features_test\n",
    "filename = 'Data/WL/features_test.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(features_test, output)\n",
    "\n",
    "# labels_test\n",
    "filename = 'Data/WL/labels_test.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(labels_test, output)\n",
    "    \n",
    "# TF-IDF object\n",
    "filename = 'Data/WL/tfidf.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(tfidf, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'business' category:\n",
      "  . Most correlated unigrams:\n",
      ". market\n",
      ". price\n",
      ". economy\n",
      ". growth\n",
      ". bank\n",
      "  . Most correlated bigrams:\n",
      ". he be\n",
      ". the us\n",
      "\n",
      "# 'entertainment' category:\n",
      "  . Most correlated unigrams:\n",
      ". music\n",
      ". best\n",
      ". star\n",
      ". award\n",
      ". film\n",
      "  . Most correlated bigrams:\n",
      ". that the\n",
      ". the government\n",
      "\n",
      "# 'politics' category:\n",
      "  . Most correlated unigrams:\n",
      ". minister\n",
      ". party\n",
      ". blair\n",
      ". election\n",
      ". labour\n",
      "  . Most correlated bigrams:\n",
      ". say he\n",
      ". the government\n",
      "\n",
      "# 'sport' category:\n",
      "  . Most correlated unigrams:\n",
      ". players\n",
      ". england\n",
      ". win\n",
      ". game\n",
      ". team\n",
      "  . Most correlated bigrams:\n",
      ". the government\n",
      ". the uk\n",
      "\n",
      "# 'tech' category:\n",
      "  . Most correlated unigrams:\n",
      ". net\n",
      ". phone\n",
      ". mobile\n",
      ". digital\n",
      ". technology\n",
      "  . Most correlated bigrams:\n",
      ". the government\n",
      ". such as\n",
      "\n",
      "Bigram : \n",
      "['go to', 'but the', 'the first', 'be in', 'would be', 'and the', 'say the', 'have be', 'to the', 'be to', 'from the', 'say it', 'over the', 'have to', 'of the', 'on the', 'in the', 'for the', 'he say', 'we be', 'the world', 'the us', 'be the', 'it be', 'want to', 'with the', 'and be', 'as the', 'there be', 'be also', 'be not', 'we have', 'last year', 'one of', 'to be', 'at the', 'the new', 'by the', 'will be', 'which be', 'say that', 'that the', 'the uk', 'up to', 'do not', 'it have', 'to get', 'they be', 'that be', 'say he', 'more than', 'to make', 'he be', 'he have', 'the government', 'such as']\n"
     ]
    }
   ],
   "source": [
    "# feature without stopwords (ws)\n",
    "df_ws = df\n",
    "\n",
    "# normalisation process\n",
    "df_ws['Content_Parsed_1'] = df_ws['Content'].str.replace(\"\\r\", \" \")\n",
    "df_ws['Content_Parsed_1'] = df_ws['Content_Parsed_1'].str.replace(\"\\n\", \" \")\n",
    "df_ws['Content_Parsed_1'] = df_ws['Content_Parsed_1'].str.replace(\"    \", \" \")\n",
    "df_ws['Content_Parsed_1'] = df_ws['Content_Parsed_1'].str.replace('\"', '')\n",
    "df_ws['Content_Parsed_2'] = df_ws['Content_Parsed_1'].str.lower()\n",
    "punctuation_signs = list(\"?:!.,;\")\n",
    "df_ws['Content_Parsed_3'] = df_ws['Content_Parsed_2']\n",
    "for punct_sign in punctuation_signs:\n",
    "    df_ws['Content_Parsed_3'] = df_ws['Content_Parsed_3'].str.replace(punct_sign, '')\n",
    "df_ws['Content_Parsed_4'] = df_ws['Content_Parsed_3'].str.replace(\"'s\", \"\")\n",
    "\n",
    "# lemillized process\n",
    "df_ws['Content_Parsed_5'] = lemillizedTextList(df_ws,'Content_Parsed_4')\n",
    "\n",
    "# reformat list\n",
    "list_columns = [\"File_Name\", \"Category\", \"Content\", \"Content_Parsed_5\"]\n",
    "df_ws = df_ws[list_columns]\n",
    "\n",
    "df_ws = df_ws.rename(columns={'Content_Parsed_5': 'Content_Parsed'})\n",
    "\n",
    "\n",
    "# labeling \n",
    "category_codes = {\n",
    "    'business': 0,\n",
    "    'entertainment': 1,\n",
    "    'politics': 2,\n",
    "    'sport': 3,\n",
    "    'tech': 4\n",
    "}\n",
    "\n",
    "# Category mapping\n",
    "df_ws['Category_Code'] = df_ws['Category']\n",
    "df_ws = df_ws.replace({'Category_Code':category_codes})\n",
    "\n",
    "# Split Data to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_ws['Content_Parsed'], \n",
    "                                                    df_ws['Category_Code'], \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=8)\n",
    "# Parameter election\n",
    "ngram_range = (1,2)\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 300\n",
    "\n",
    "# create tfId\n",
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "\n",
    "# define feature train\n",
    "features_train = tfidf.fit_transform(X_train).toarray()\n",
    "labels_train = y_train\n",
    "\n",
    "# define feature test\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "labels_test = y_test\n",
    "\n",
    "# define unigram and bigram\n",
    "for Product, category_id in sorted(category_codes.items()):\n",
    "    features_chi2 = chi2(features_train, labels_train == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    print(\"# '{}' category:\".format(Product))\n",
    "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-5:])))\n",
    "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-2:])))\n",
    "    print(\"\")\n",
    "   \n",
    "print(\"Bigram : \")\n",
    "print(bigrams)\n",
    "\n",
    "# Saved output \n",
    "# X_train\n",
    "filename = 'Data/WS/X_train.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(X_train, output)\n",
    "    \n",
    "# X_test    \n",
    "filename = 'Data/WS/X_test.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(X_test, output)\n",
    "    \n",
    "# y_train\n",
    "filename = 'Data/WS/y_train.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(y_train, output)\n",
    "    \n",
    "# y_test\n",
    "filename = 'Data/WS/y_test.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(y_test, output)\n",
    "    \n",
    "# df\n",
    "filename = 'Data/WS/df_ws.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(df_ws, output)\n",
    "    \n",
    "# features_train\n",
    "filename = 'Data/WS/features_train.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(features_train, output)\n",
    "\n",
    "# labels_train\n",
    "filename = 'Data/WS/labels_train.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(labels_train, output)\n",
    "\n",
    "# features_test\n",
    "filename = 'Data/WS/features_test.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(features_test, output)\n",
    "\n",
    "# labels_test\n",
    "filename = 'Data/WS/labels_test.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(labels_test, output)\n",
    "    \n",
    "# TF-IDF object\n",
    "filename = 'Data/WS/tfidf.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(tfidf, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'business' category:\n",
      "  . Most correlated unigrams:\n",
      ". price\n",
      ". market\n",
      ". economy\n",
      ". growth\n",
      ". bank\n",
      "  . Most correlated bigrams:\n",
      ". say mr\n",
      ". last year\n",
      "\n",
      "# 'entertainment' category:\n",
      "  . Most correlated unigrams:\n",
      ". tv\n",
      ". music\n",
      ". star\n",
      ". award\n",
      ". film\n",
      "  . Most correlated bigrams:\n",
      ". last year\n",
      ". say mr\n",
      "\n",
      "# 'politics' category:\n",
      "  . Most correlated unigrams:\n",
      ". minister\n",
      ". election\n",
      ". blair\n",
      ". party\n",
      ". labour\n",
      "  . Most correlated bigrams:\n",
      ". last year\n",
      ". say mr\n",
      "\n",
      "# 'sport' category:\n",
      "  . Most correlated unigrams:\n",
      ". win\n",
      ". side\n",
      ". game\n",
      ". team\n",
      ". match\n",
      "  . Most correlated bigrams:\n",
      ". last year\n",
      ". say mr\n",
      "\n",
      "# 'tech' category:\n",
      "  . Most correlated unigrams:\n",
      ". mobile\n",
      ". digital\n",
      ". technology\n",
      ". software\n",
      ". users\n",
      "  . Most correlated bigrams:\n",
      ". last year\n",
      ". say mr\n",
      "\n",
      "Bigram : \n",
      "['last year', 'say mr']\n"
     ]
    }
   ],
   "source": [
    "# TFIDF Small (tfidf_s)\n",
    "df_s = df\n",
    "\n",
    "# labeling \n",
    "category_codes = {\n",
    "    'business': 0,\n",
    "    'entertainment': 1,\n",
    "    'politics': 2,\n",
    "    'sport': 3,\n",
    "    'tech': 4\n",
    "}\n",
    "\n",
    "# Category mapping\n",
    "df_s['Category_Code'] = df_s['Category']\n",
    "df_s = df_s.replace({'Category_Code':category_codes})\n",
    "\n",
    "# Split Data to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_s['Content_Parsed'], \n",
    "                                                    df_s['Category_Code'], \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=8)\n",
    "# Parameter election with 200 max features\n",
    "ngram_range = (1,2)\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 200\n",
    "\n",
    "# create tfId\n",
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "\n",
    "# define feature train\n",
    "features_train = tfidf.fit_transform(X_train).toarray()\n",
    "labels_train = y_train\n",
    "\n",
    "# define feature test\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "labels_test = y_test\n",
    "\n",
    "# define unigram and bigram\n",
    "for Product, category_id in sorted(category_codes.items()):\n",
    "    features_chi2 = chi2(features_train, labels_train == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    print(\"# '{}' category:\".format(Product))\n",
    "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-5:])))\n",
    "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-2:])))\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Bigram : \")\n",
    "print(bigrams)\n",
    "\n",
    "# Saved output \n",
    "# X_train\n",
    "filename = 'Data/TFIDFS/X_train.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(X_train, output)\n",
    "    \n",
    "# X_test    \n",
    "filename = 'Data/TFIDFS/X_test.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(X_test, output)\n",
    "    \n",
    "# y_train\n",
    "filename = 'Data/TFIDFS/y_train.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(y_train, output)\n",
    "    \n",
    "# y_test\n",
    "filename = 'Data/TFIDFS/y_test.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(y_test, output)\n",
    "    \n",
    "# df\n",
    "filename = 'Data/TFIDFS/df_s.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(df_s, output)\n",
    "    \n",
    "# features_train\n",
    "filename = 'Data/TFIDFS/features_train.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(features_train, output)\n",
    "\n",
    "# labels_train\n",
    "filename = 'Data/TFIDFS/labels_train.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(labels_train, output)\n",
    "\n",
    "# features_test\n",
    "filename = 'Data/TFIDFS/features_test.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(features_test, output)\n",
    "\n",
    "# labels_test\n",
    "filename = 'Data/TFIDFS/labels_test.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(labels_test, output)\n",
    "    \n",
    "# TF-IDF object\n",
    "filename = 'Data/TFIDFS/tfidf.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(tfidf, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'business' category:\n",
      "  . Most correlated unigrams:\n",
      ". firm\n",
      ". economy\n",
      ". growth\n",
      ". oil\n",
      ". bank\n",
      "  . Most correlated bigrams:\n",
      ". last year\n",
      ". year old\n",
      "\n",
      "# 'entertainment' category:\n",
      "  . Most correlated unigrams:\n",
      ". music\n",
      ". band\n",
      ". star\n",
      ". award\n",
      ". film\n",
      "  . Most correlated bigrams:\n",
      ". mr blair\n",
      ". prime minister\n",
      "\n",
      "# 'politics' category:\n",
      "  . Most correlated unigrams:\n",
      ". tory\n",
      ". blair\n",
      ". party\n",
      ". election\n",
      ". labour\n",
      "  . Most correlated bigrams:\n",
      ". prime minister\n",
      ". mr blair\n",
      "\n",
      "# 'sport' category:\n",
      "  . Most correlated unigrams:\n",
      ". season\n",
      ". champion\n",
      ". cup\n",
      ". coach\n",
      ". match\n",
      "  . Most correlated bigrams:\n",
      ". say mr\n",
      ". year old\n",
      "\n",
      "# 'tech' category:\n",
      "  . Most correlated unigrams:\n",
      ". digital\n",
      ". technology\n",
      ". computer\n",
      ". software\n",
      ". users\n",
      "  . Most correlated bigrams:\n",
      ". year old\n",
      ". say mr\n",
      "\n",
      "Bigram : \n",
      "['tell bbc', 'last year', 'mr brown', 'prime minister', 'mr blair', 'year old', 'say mr']\n"
     ]
    }
   ],
   "source": [
    "# TFIDF High (tfidf_h)\n",
    "df_h = df\n",
    "\n",
    "# labeling \n",
    "category_codes = {\n",
    "    'business': 0,\n",
    "    'entertainment': 1,\n",
    "    'politics': 2,\n",
    "    'sport': 3,\n",
    "    'tech': 4\n",
    "}\n",
    "\n",
    "# Category mapping\n",
    "df_h['Category_Code'] = df_h['Category']\n",
    "df_h = df_h.replace({'Category_Code':category_codes})\n",
    "\n",
    "# Split Data to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_h['Content_Parsed'], \n",
    "                                                    df_h['Category_Code'], \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=8)\n",
    "# Parameter election with 400 max features\n",
    "ngram_range = (1,2)\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 400\n",
    "\n",
    "# create tfId\n",
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "\n",
    "# define feature train\n",
    "features_train = tfidf.fit_transform(X_train).toarray()\n",
    "labels_train = y_train\n",
    "\n",
    "# define feature test\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "labels_test = y_test\n",
    "\n",
    "# define unigram and bigram\n",
    "for Product, category_id in sorted(category_codes.items()):\n",
    "    features_chi2 = chi2(features_train, labels_train == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    print(\"# '{}' category:\".format(Product))\n",
    "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-5:])))\n",
    "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-2:])))\n",
    "    print(\"\")\n",
    "    \n",
    "print(\"Bigram : \")\n",
    "print(bigrams)\n",
    "\n",
    "# Saved output \n",
    "# X_train\n",
    "filename = 'Data/TFIDFH/X_train.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(X_train, output)\n",
    "    \n",
    "# X_test    \n",
    "filename = 'Data/TFIDFH/X_test.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(X_test, output)\n",
    "    \n",
    "# y_train\n",
    "filename = 'Data/TFIDFH/y_train.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(y_train, output)\n",
    "    \n",
    "# y_test\n",
    "filename = 'Data/TFIDFH/y_test.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(y_test, output)\n",
    "    \n",
    "# df\n",
    "filename = 'Data/TFIDFH/df_h.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(df_h, output)\n",
    "    \n",
    "# features_train\n",
    "filename = 'Data/TFIDFH/features_train.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(features_train, output)\n",
    "\n",
    "# labels_train\n",
    "filename = 'Data/TFIDFH/labels_train.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(labels_train, output)\n",
    "\n",
    "# features_test\n",
    "filename = 'Data/TFIDFH/features_test.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(features_test, output)\n",
    "\n",
    "# labels_test\n",
    "filename = 'Data/TFIDFH/labels_test.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(labels_test, output)\n",
    "    \n",
    "# TF-IDF object\n",
    "filename = 'Data/TFIDFH/tfidf.pickle'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as output:\n",
    "    pickle.dump(tfidf, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use in Multinomial Naive Bayes:\n",
      "\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters from Grid Search are:\n",
      "{'alpha': 0.1, 'class_prior': None, 'fit_prior': False}\n",
      "\n",
      "The mean accuracy of a model with these hyperparameters is:\n",
      "0.9573333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "# Algoritma Multinomial Naive Bayes (model 2)\n",
    "choice = 2\n",
    "\n",
    "# Cross Validation\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "print('Parameters currently in use in {}:\\n'.format(models[choice]))\n",
    "pprint(classifier)\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {'alpha': [float(x) for x in np.linspace(start = 0.1, stop = 1.0, num = 10)],\n",
    "               'class_prior': [None],\n",
    "               'fit_prior': [True, False]}\n",
    "\n",
    "# Manually create the splits in CV in order to be able to fix a random_state (GridSearchCV doesn't have that argument)\n",
    "cv_sets = ShuffleSplit(n_splits = 3, test_size = .33, random_state = 8)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=classifier, \n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=cv_sets,\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(features_train, labels_train)\n",
    "\n",
    "# Result Tunning param using Grid Search\n",
    "print(\"The best hyperparameters from Grid Search are:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use in K Nearest Neighbour:\n",
      "\n",
      "{'algorithm': 'auto',\n",
      " 'leaf_size': 30,\n",
      " 'metric': 'minkowski',\n",
      " 'metric_params': None,\n",
      " 'n_jobs': None,\n",
      " 'n_neighbors': 5,\n",
      " 'p': 2,\n",
      " 'weights': 'uniform'}\n",
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 216 out of 216 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters from Grid Search are:\n",
      "{'leaf_size': 10, 'n_neighbors': 5, 'p': 2}\n",
      "\n",
      "The mean accuracy of a model with these hyperparameters is:\n",
      "0.9578666666666668\n"
     ]
    }
   ],
   "source": [
    "# Algoritma K Nearest Neighbour (model 3)\n",
    "choice = 3\n",
    "\n",
    "classifier = KNeighborsClassifier()\n",
    "\n",
    "print('Parameters currently in use in {}:\\n'.format(models[choice]))\n",
    "pprint(classifier.get_params())\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'leaf_size' : [10,20,30,40,50,60],\n",
    "    'n_neighbors' : [5,10,20,30,40,50],\n",
    "    'p' : [1,2]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Create a base model\n",
    "classifier = KNeighborsClassifier()\n",
    "\n",
    "# Manually create the splits in CV in order to be able to fix a random_state (GridSearchCV doesn't have that argument)\n",
    "cv_sets = ShuffleSplit(n_splits = 3, test_size = .33, random_state = 8)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=classifier, \n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=cv_sets,\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(features_train, labels_train)\n",
    "\n",
    "# Result Tunning param using Grid Search\n",
    "print(\"The best hyperparameters from Grid Search are:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use in Support Vector Machines:\n",
      "\n",
      "{'C': 1.0,\n",
      " 'break_ties': False,\n",
      " 'cache_size': 200,\n",
      " 'class_weight': None,\n",
      " 'coef0': 0.0,\n",
      " 'decision_function_shape': 'ovr',\n",
      " 'degree': 3,\n",
      " 'gamma': 'scale',\n",
      " 'kernel': 'rbf',\n",
      " 'max_iter': -1,\n",
      " 'probability': False,\n",
      " 'random_state': 8,\n",
      " 'shrinking': True,\n",
      " 'tol': 0.001,\n",
      " 'verbose': False}\n",
      "{'C': [0.0001, 0.001, 0.01],\n",
      " 'degree': [1, 2, 3, 4, 5],\n",
      " 'gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
      " 'kernel': ['linear', 'rbf', 'poly']}\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:  4.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters from Random Search are:\n",
      "{'kernel': 'poly', 'gamma': 10, 'degree': 4, 'C': 0.01}\n",
      "\n",
      "The mean accuracy of a model with these hyperparameters is:\n",
      "0.9201502611962201\n",
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:  6.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters from Grid Search are:\n",
      "{'C': 1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "\n",
      "The mean accuracy of a model with these hyperparameters is:\n",
      "0.9722666666666666\n"
     ]
    }
   ],
   "source": [
    "# Algoritma Support Vector Machines (model 4)\n",
    "choice = 4\n",
    "\n",
    "classifier = svm.SVC(random_state=8)\n",
    "\n",
    "print('Parameters currently in use in {}:\\n'.format(models[choice]))\n",
    "pprint(classifier.get_params())\n",
    "\n",
    "# Create the random grid SVM\n",
    "random_grid = {'C': [.0001, .001, .01],\n",
    "              'kernel': ['linear', 'rbf', 'poly'],\n",
    "              'gamma': [.0001, .001, .01, .1, 1, 10, 100],\n",
    "              'degree': [1, 2, 3, 4, 5]\n",
    "             }\n",
    "\n",
    "pprint(random_grid)\n",
    "\n",
    "\n",
    "# Definition of the random search\n",
    "random_search = RandomizedSearchCV(estimator=classifier,\n",
    "                                   param_distributions=random_grid,\n",
    "                                   n_iter=50,\n",
    "                                   scoring='accuracy',\n",
    "                                   cv=3, \n",
    "                                   verbose=1, \n",
    "                                   random_state=8)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(features_train, labels_train)\n",
    "\n",
    "# Result Tunning param using Random Search\n",
    "print(\"The best hyperparameters from Random Search are:\")\n",
    "print(random_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(random_search.best_score_)\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "               'kernel': ['rbf','linear','poly','sigmoid'],\n",
    "               'gamma' : [1,0.1,0.01,0.001]}\n",
    "\n",
    "# Manually create the splits in CV in order to be able to fix a random_state (GridSearchCV doesn't have that argument)\n",
    "cv_sets = ShuffleSplit(n_splits = 3, test_size = .33, random_state = 8)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=classifier, \n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=cv_sets,\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(features_train, labels_train)\n",
    "\n",
    "# Result Tunning param using Grid Search\n",
    "print(\"The best hyperparameters from Grid Search are:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use in Random Forest:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': 8,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n",
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [20, 40, 60, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 4],\n",
      " 'min_samples_split': [2, 5],\n",
      " 'n_estimators': [50, 100, 150, 200]}\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:  2.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters from Random Search are:\n",
      "{'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20, 'bootstrap': False}\n",
      "\n",
      "The mean accuracy of a model with these hyperparameters is:\n",
      "0.9423590672402083\n",
      "Fitting 3 folds for each of 256 candidates, totalling 768 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 768 out of 768 | elapsed: 15.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters from Grid Search are:\n",
      "{'bootstrap': False, 'max_depth': 40, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "\n",
      "The mean accuracy of a model with these hyperparameters is:\n",
      "0.9429333333333333\n"
     ]
    }
   ],
   "source": [
    "# Algoritma Random Forest (model 5)\n",
    "choice = 5\n",
    "\n",
    "classifier = RandomForestClassifier(random_state = 8)\n",
    "print('Parameters currently in use in {}:\\n'.format(models[choice]))\n",
    "pprint(classifier.get_params())\n",
    "\n",
    "# Create the random grid Random Forest\n",
    "random_grid = {'n_estimators': [50, 100, 150, 200],\n",
    "           'max_features': ['auto', 'sqrt'],\n",
    "           'max_depth': [20, 40, 60, None],\n",
    "           'min_samples_split': [2, 5],\n",
    "           'min_samples_leaf': [1, 4],\n",
    "           'bootstrap': [True, False]}\n",
    "    \n",
    "pprint(random_grid)\n",
    "\n",
    "# Definition of the random search\n",
    "random_search = RandomizedSearchCV(estimator=classifier,\n",
    "                                   param_distributions=random_grid,\n",
    "                                   n_iter=50,\n",
    "                                   scoring='accuracy',\n",
    "                                   cv=3, \n",
    "                                   verbose=1, \n",
    "                                   random_state=8)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(features_train, labels_train)\n",
    "\n",
    "# Result Tunning param using Random Search\n",
    "print(\"The best hyperparameters from Random Search are:\")\n",
    "print(random_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(random_search.best_score_)\n",
    "\n",
    "# parameter\n",
    "# Number of trees in random forest\n",
    "n_estimators = [50, 100, 150, 200]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [20, 40, 60, None]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [False, True]\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# Create a base model\n",
    "classifier = RandomForestClassifier(random_state=8)\n",
    "\n",
    "# Manually create the splits in CV in order to be able to fix a random_state (GridSearchCV doesn't have that argument)\n",
    "cv_sets = ShuffleSplit(n_splits = 3, test_size = .33, random_state = 8)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=classifier, \n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=cv_sets,\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(features_train, labels_train)\n",
    "\n",
    "# Result Tunning param using Grid Search\n",
    "print(\"The best hyperparameters from Grid Search are:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the stop words in indonesia\n",
    "# stop_words_id = list(stopwords.words('indonesia'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
